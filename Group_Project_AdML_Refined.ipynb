{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e981ef55",
   "metadata": {},
   "source": [
    "# Competition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09704a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba1e4ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train raw: (432600, 18)\n",
      "Test raw:  (103500, 16)\n"
     ]
    }
   ],
   "source": [
    "train_raw = pd.read_csv(\"train_competition_2026.csv\")\n",
    "test_raw  = pd.read_csv(\"test_no_outcome.csv\")\n",
    "\n",
    "train_raw[\"time\"] = pd.to_datetime(train_raw[\"time\"])\n",
    "test_raw[\"time\"]  = pd.to_datetime(test_raw[\"time\"])\n",
    "\n",
    "print(f\"Train raw: {train_raw.shape}\")\n",
    "print(f\"Test raw:  {test_raw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e426d803",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "Each observation has 30 timepoints across 5 signals (t_0 to t_4). We collapse each observation into one row with simple summary stats: mean, std, min, max, first, last, slope, range, quantiles, skewness, and kurtosis. We also add time-based features and a few interaction features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88934815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    t_cols = [f\"t_{i}\" for i in range(5)]\n",
    "    num_cols = [\"num_0\", \"num_1\", \"num_2\"]\n",
    "    cat_cols = [f\"cat_{i}\" for i in range(5)]\n",
    "\n",
    "    df = df.sort_values([\"obs\", \"time\"]).copy()\n",
    "\n",
    "    # Step 1: Basic aggregations per observation\n",
    "    agg_dict = {}\n",
    "    for c in num_cols + cat_cols:\n",
    "        agg_dict[c] = \"first\"\n",
    "    agg_dict[\"sub_id\"] = \"first\"\n",
    "    for c in t_cols:\n",
    "        agg_dict[c] = [\"mean\", \"std\", \"min\", \"max\", \"first\", \"last\", \"median\"]\n",
    "\n",
    "    grouped = df.groupby(\"obs\").agg(agg_dict)\n",
    "    grouped.columns = [\"_\".join(col).strip(\"_\") for col in grouped.columns]\n",
    "    grouped = grouped.reset_index()\n",
    "\n",
    "    # Step 2: Derived stats from the basic aggregations\n",
    "    for c in t_cols:\n",
    "        grouped[f\"{c}_slope\"] = grouped[f\"{c}_last\"] - grouped[f\"{c}_first\"]\n",
    "        grouped[f\"{c}_range\"] = grouped[f\"{c}_max\"] - grouped[f\"{c}_min\"]\n",
    "        grouped[f\"{c}_cv\"] = grouped[f\"{c}_std\"] / (grouped[f\"{c}_mean\"].abs() + 1e-8)\n",
    "\n",
    "    # Step 3: Quantiles\n",
    "    quantile_feats = df.groupby(\"obs\")[t_cols].quantile([0.1, 0.25, 0.75, 0.9])\n",
    "    quantile_feats = quantile_feats.unstack(level=-1)\n",
    "    quantile_feats.columns = [f\"{c}_q{int(q*100)}\" for c, q in quantile_feats.columns]\n",
    "    grouped = grouped.merge(quantile_feats.reset_index(), on=\"obs\")\n",
    "\n",
    "    # Step 4: Skewness and kurtosis\n",
    "    skew_feats = df.groupby(\"obs\")[t_cols].skew()\n",
    "    skew_feats.columns = [f\"{c}_skew\" for c in t_cols]\n",
    "    grouped = grouped.merge(skew_feats.reset_index(), on=\"obs\")\n",
    "\n",
    "    kurt_feats = df.groupby(\"obs\")[t_cols].apply(lambda x: x.kurtosis())\n",
    "    kurt_feats.columns = [f\"{c}_kurt\" for c in t_cols]\n",
    "    grouped = grouped.merge(kurt_feats.reset_index(), on=\"obs\")\n",
    "\n",
    "    # Step 5: Interaction features between signals\n",
    "    grouped[\"t0_minus_t1\"] = grouped[\"t_0_mean\"] - grouped[\"t_1_mean\"]\n",
    "    grouped[\"t2_minus_t3\"] = grouped[\"t_2_mean\"] - grouped[\"t_3_mean\"]\n",
    "    grouped[\"t_mean_all\"] = grouped[[f\"t_{i}_mean\" for i in range(5)]].mean(axis=1)\n",
    "\n",
    "    # Step 6: Interaction features between static numerics\n",
    "    grouped[\"num0_times_num1\"] = grouped[\"num_0_first\"] * grouped[\"num_1_first\"]\n",
    "    grouped[\"num0_times_num2\"] = grouped[\"num_0_first\"] * grouped[\"num_2_first\"]\n",
    "\n",
    "    # Step 7: Time features\n",
    "    time_feats = df.groupby(\"obs\")[\"time\"].first()\n",
    "    grouped[\"hour\"] = pd.to_datetime(time_feats.values).hour\n",
    "    grouped[\"dayofweek\"] = pd.to_datetime(time_feats.values).dayofweek\n",
    "    grouped[\"is_weekend\"] = (grouped[\"dayofweek\"] >= 5).astype(int)\n",
    "\n",
    "    # Step 8: How many observations each subject has\n",
    "    sub_counts = df.groupby(\"sub_id\")[\"obs\"].nunique().reset_index()\n",
    "    sub_counts.columns = [\"sub_id\", \"sub_obs_count\"]\n",
    "    grouped = grouped.merge(sub_counts, left_on=\"sub_id_first\", right_on=\"sub_id\", how=\"left\")\n",
    "    grouped = grouped.drop(columns=[\"sub_id\"])\n",
    "\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30446f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features...\n",
      "Train aggregated: (14420, 199)\n",
      "Test aggregated:  (3450, 197)\n"
     ]
    }
   ],
   "source": [
    "train_agg = engineer_features(train_raw)\n",
    "test_agg  = engineer_features(test_raw)\n",
    "\n",
    "targets = train_raw.groupby(\"obs\")[[\"y_1\", \"y_2\"]].first().reset_index()\n",
    "train_agg = train_agg.merge(targets, on=\"obs\")\n",
    "\n",
    "print(f\"Train: {train_agg.shape}\")\n",
    "print(f\"Test:  {test_agg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18082f94",
   "metadata": {},
   "source": [
    "## 3. Prepare Features\n",
    "\n",
    "Encode categorical columns as numbers using LabelEncoder, select all feature columns, and separate out the targets (y_1, y_2) and group IDs (sub_id) for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9959623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"obs\", \"sub_id_first\", \"y_1\", \"y_2\"]\n",
    "feature_cols = [c for c in train_agg.columns if c not in drop_cols]\n",
    "\n",
    "cat_features_raw = [f\"cat_{i}_first\" for i in range(5)]\n",
    "label_encoders = {}\n",
    "for c in cat_features_raw:\n",
    "    le = LabelEncoder()\n",
    "    all_vals = pd.concat([train_agg[c], test_agg[c]]).astype(str)\n",
    "    le.fit(all_vals)\n",
    "    train_agg[c] = le.transform(train_agg[c].astype(str))\n",
    "    test_agg[c]  = le.transform(test_agg[c].astype(str))\n",
    "    label_encoders[c] = le\n",
    "\n",
    "X_all = train_agg[feature_cols].copy()\n",
    "y_all = train_agg[[\"y_1\", \"y_2\"]].copy()\n",
    "groups = train_agg[\"sub_id_first\"].values\n",
    "X_test = test_agg[feature_cols].copy()\n",
    "\n",
    "X_all = X_all.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(f\"Feature count: {len(feature_cols)}\")\n",
    "print(f\"Train obs: {len(X_all)}, Test obs: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6603ccf",
   "metadata": {},
   "source": [
    "## 4. Train Models (LightGBM + XGBoost + CatBoost)\n",
    "\n",
    "We train 3 different models and average their predictions. We use GroupKFold so the same subject is never in both train and validation at the same time. We repeat with 3 random seeds to make results more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7774812",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 7\n",
    "SEEDS = [42, 123, 2026]\n",
    "TARGETS = [\"y_1\", \"y_2\"]\n",
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "\n",
    "n_train, n_test = len(X_all), len(X_test)\n",
    "n_seeds = len(SEEDS)\n",
    "\n",
    "# Store out of fold and test predictions for each model and target\n",
    "oof_preds = {model: {t: np.zeros(n_train) for t in TARGETS} for model in [\"lgb\", \"xgb\", \"cat\"]}\n",
    "test_preds = {model: {t: np.zeros(n_test) for t in TARGETS} for model in [\"lgb\", \"xgb\", \"cat\"]}\n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(f\"SEED {seed}\")\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(gkf.split(X_all, y_all, groups)):\n",
    "        print(f\"  Fold {fold+1}/{N_FOLDS}\")\n",
    "        X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "        y_tr, y_va = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "\n",
    "        for target in TARGETS:\n",
    "            # LightGBM\n",
    "            m_lgb = lgb.LGBMRegressor(\n",
    "                objective=\"mae\", metric=\"mae\", verbosity=-1,\n",
    "                n_estimators=3000, learning_rate=0.02, num_leaves=63,\n",
    "                min_child_samples=50, subsample=0.7, colsample_bytree=0.6,\n",
    "                reg_alpha=0.5, reg_lambda=5.0, random_state=seed, n_jobs=-1, subsample_freq=1,\n",
    "            )\n",
    "            m_lgb.fit(X_tr, y_tr[target], eval_set=[(X_va, y_va[target])],\n",
    "                      callbacks=[lgb.early_stopping(150, verbose=False), lgb.log_evaluation(0)])\n",
    "            oof_preds[\"lgb\"][target][va_idx] += m_lgb.predict(X_va)\n",
    "            test_preds[\"lgb\"][target] += m_lgb.predict(X_test) / (N_FOLDS * n_seeds)\n",
    "\n",
    "            # XGBoost\n",
    "            m_xgb = xgb.XGBRegressor(\n",
    "                objective=\"reg:absoluteerror\", eval_metric=\"mae\",\n",
    "                n_estimators=3000, learning_rate=0.02, max_depth=6,\n",
    "                min_child_weight=50, subsample=0.7, colsample_bytree=0.6,\n",
    "                reg_alpha=0.5, reg_lambda=5.0, random_state=seed, n_jobs=-1, verbosity=0, tree_method=\"hist\",\n",
    "            )\n",
    "            m_xgb.fit(X_tr, y_tr[target], eval_set=[(X_va, y_va[target])], verbose=False)\n",
    "            oof_preds[\"xgb\"][target][va_idx] += m_xgb.predict(X_va)\n",
    "            test_preds[\"xgb\"][target] += m_xgb.predict(X_test) / (N_FOLDS * n_seeds)\n",
    "\n",
    "            # CatBoost\n",
    "            m_cat = CatBoostRegressor(\n",
    "                loss_function=\"MAE\", iterations=3000, learning_rate=0.03, depth=6,\n",
    "                l2_leaf_reg=5.0, min_data_in_leaf=50, random_seed=seed, verbose=0, subsample=0.7,\n",
    "            )\n",
    "            m_cat.fit(X_tr, y_tr[target], eval_set=(X_va, y_va[target]), early_stopping_rounds=150)\n",
    "            oof_preds[\"cat\"][target][va_idx] += m_cat.predict(X_va)\n",
    "            test_preds[\"cat\"][target] += m_cat.predict(X_test) / (N_FOLDS * n_seeds)\n",
    "\n",
    "# Average OOF predictions across seeds\n",
    "for model in oof_preds:\n",
    "    for target in TARGETS:\n",
    "        oof_preds[model][target] /= n_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93ded21",
   "metadata": {},
   "source": [
    "## 5. Evaluate and Optimize Ensemble Weights\n",
    "\n",
    "Check how each model did on its own, then find the best way to combine them. We search for weights (adding up to 1) that minimize MAE when blending the 3 models together. We do this separately for y_1 and y_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdaebcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_true = y_all[\"y_1\"].values\n",
    "y2_true = y_all[\"y_2\"].values\n",
    "\n",
    "print(\"Individual Model Scores\")\n",
    "for name, key in [(\"LightGBM\", \"lgb\"), (\"XGBoost\", \"xgb\"), (\"CatBoost\", \"cat\")]:\n",
    "    m1 = mean_absolute_error(y1_true, oof_preds[key][\"y_1\"])\n",
    "    m2 = mean_absolute_error(y2_true, oof_preds[key][\"y_2\"])\n",
    "    print(f\"  {name:10s} → y1: {m1:.4f}, y2: {m2:.4f}, avg: {(m1+m2)/2:.4f}\")\n",
    "\n",
    "def find_weights(oof_list, y_true):\n",
    "    n = len(oof_list)\n",
    "    def obj(w):\n",
    "        blend = sum(w[i] * oof_list[i] for i in range(n))\n",
    "        return mean_absolute_error(y_true, blend)\n",
    "    return minimize(obj, x0=[1/n]*n, method=\"SLSQP\",\n",
    "                    bounds=[(0,1)]*n,\n",
    "                    constraints={\"type\": \"eq\", \"fun\": lambda w: sum(w) - 1})\n",
    "\n",
    "res_y1 = find_weights([oof_preds[m][\"y_1\"] for m in [\"lgb\", \"xgb\", \"cat\"]], y1_true)\n",
    "res_y2 = find_weights([oof_preds[m][\"y_2\"] for m in [\"lgb\", \"xgb\", \"cat\"]], y2_true)\n",
    "w_y1, w_y2 = res_y1.x, res_y2.x\n",
    "\n",
    "print(f\"Optimal Weights (LGB / XGB / CAT)\")\n",
    "print(f\"  y_1: [{w_y1[0]:.3f}, {w_y1[1]:.3f}, {w_y1[2]:.3f}] → MAE: {res_y1.fun:.4f}\")\n",
    "print(f\"  y_2: [{w_y2[0]:.3f}, {w_y2[1]:.3f}, {w_y2[2]:.3f}] → MAE: {res_y2.fun:.4f}\")\n",
    "print(f\"  Avg MAE: {(res_y1.fun + res_y2.fun)/2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d96dc",
   "metadata": {},
   "source": [
    "## 6. Generate Submission\n",
    "\n",
    "Combine the 3 models' test predictions using the optimized weights from step 5, then save to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b33336",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"lgb\", \"xgb\", \"cat\"]\n",
    "\n",
    "final_y1 = sum(w_y1[i] * test_preds[m][\"y_1\"] for i, m in enumerate(models))\n",
    "final_y2 = sum(w_y2[i] * test_preds[m][\"y_2\"] for i, m in enumerate(models))\n",
    "\n",
    "submission = pd.DataFrame({\"obs\": test_agg[\"obs\"], \"y_1\": final_y1, \"y_2\": final_y2})\n",
    "submission.to_csv(\"sample_submission3.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(submission)} rows to sample_submission3.csv\")\n",
    "submission.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
